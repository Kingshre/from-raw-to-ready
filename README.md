# From Raw to Ready â€” End-to-End ML Data Pipeline

A production-style machine learning data pipeline that turns **messy raw data**
into **validated, versioned, model-ready features** using **Python, SQL, and Docker**.

This project demonstrates real-world ML/data engineering practices:
data quality gates, reproducibility, feature versioning, and time-aware splits.

---

## ğŸ— Architecture

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Raw CSV Data â”‚
â”‚ (data/raw) â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚
â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Ingestion â”‚
â”‚ raw.raw_events â”‚
â”‚ (immutable JSON events) â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚
â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Validation â”‚
â”‚ configs/expectations â”‚
â”‚ â†’ artifacts/reports â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚
â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Staging â”‚
â”‚ stg.orders â”‚
â”‚ (clean + deduplicated) â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚
â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Feature Engineering â”‚
â”‚ sql/marts.sql â”‚
â”‚ feats.customer_features â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚
â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Dataset Splits â”‚
â”‚ meta.dataset_splits â”‚
â”‚ (train / val / test) â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚
â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Feature Registry â”‚
â”‚ meta.feature_registry â”‚
â”‚ (versioning + lineage) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

markdown
Copy code

---

## ğŸ” Data Flow (Step-by-Step)

1. **Ingest**
   - Reads raw CSV files
   - Stores each row as an immutable JSON record in Postgres
   - Table: `raw.raw_events`

2. **Validate**
   - Applies schema, null, range, and uniqueness checks
   - Rules defined in `configs/expectations.yaml`
   - Writes a machine-readable validation report
   - Fails fast if data quality issues are found

3. **Stage**
   - Cleans, deduplicates, and standardizes data
   - Output table: `stg.orders`

4. **Feature Engineering**
   - SQL-based rolling aggregations
   - Output table: `feats.customer_features`
   - Time-aware feature computation

5. **Splits & Registry**
   - Creates train / validation / test splits
   - Logs feature version, config hash, and SQL hash
   - Tables: `meta.dataset_splits`, `meta.feature_registry`

---

## ğŸ§ª Validation Example

When data quality issues exist, the pipeline **stops automatically** and produces a report:

```json
{
  "validation_passed": false,
  "errors": [
    "order_id has 1 duplicate(s)",
    "amount has 1 < 0",
    "order_ts has 1 invalid timestamp(s)"
  ]
}
